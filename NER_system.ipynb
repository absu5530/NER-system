{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading train file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readconll(file):\n",
    "    lines = [line.strip() for line in open(file)]\n",
    "    while lines[-1] == '':  # Remove trailing empty lines\n",
    "        lines.pop()\n",
    "    s = [x.split('_') for x in '_'.join(lines).split('__')]  # Quick split corpus into sentences\n",
    "    return [[y.split() for y in x] for x in s]\n",
    "\n",
    "sentences = readconll('eng.train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting features and classes and training classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature extraction function\n",
    "def featurizer(sentences):\n",
    "    features = []\n",
    "    for sentence in sentences:\n",
    "        token_count = 0\n",
    "        for token in sentence:\n",
    "            token_features = {}\n",
    "            \n",
    "            token_features['word#' + token[0]] = 1 #lexical identity\n",
    "            \n",
    "            token_features['pos#' + token[1]] = 1 #POS tag\n",
    "            \n",
    "            if token_count == 0: #first word of sentence/initial capitalization\n",
    "                if token[0][0].isupper() == True:\n",
    "                    token_features['firstword-initcaps#'] = 1\n",
    "                else:\n",
    "                    token_features['firstword-notinitcaps#'] = 1\n",
    "            else:\n",
    "                if token[0][0].isupper() == True:\n",
    "                    token_features['initcaps#'] = 1\n",
    "            \n",
    "            if token[0].isupper() == True: #all caps\n",
    "                token_features['allcaps#'] = 1\n",
    "            \n",
    "            if token[0][0].islower() == True and token[0].upper() != token[0]: #mixed caps\n",
    "                token_features['mixedcaps#'] = 1\n",
    "            \n",
    "            if token_count > 0: #preceding token's lexical identity, POS tag and word shape\n",
    "                token_features['preceding_word#' + sentence[token_count - 1][0]] = 1\n",
    "                token_features['preceding_pos#' + sentence[token_count - 1][1]] = 1\n",
    "                token_features['preceding_word_shape#' + ''.join(['x' if character.islower() == True else 'X' if character.isupper() == True else 'd' if character.isnumeric() == True else character for character in sentence[token_count-1][0]])] = 1\n",
    "            \n",
    "            if token_count < len(sentence) - 1: #succeeding token's lexical identity, POS tag and word shape\n",
    "                token_features['succeeding_word#' + sentence[token_count + 1][0]] = 1\n",
    "                token_features['succeeding_pos#' + sentence[token_count + 1][1]] = 1\n",
    "                token_features['succeeding_word_shape#' + ''.join(['x' if character.islower() == True else 'X' if character.isupper() == True else 'd' if character.isnumeric() == True else character for character in sentence[token_count+1][0]])] = 1\n",
    "\n",
    "            c = 1 #prefixes and suffixes up to four characters\n",
    "            while c != len(token[0]) and c < 5:\n",
    "                p = 'pre' + str(c) + '#'\n",
    "                s = 'suff' + str(c) + '#'\n",
    "                p = p + token[0][:c]\n",
    "                s = s + token[0][c:]\n",
    "                c += 1\n",
    "                token_features[p] = 1\n",
    "                token_features[s] = 1\n",
    "            \n",
    "            word_shape = ''.join(['x' if character.islower() == True else 'X' if character.isupper() == True else 'd' if character.isnumeric() == True else character for character in token[0]])\n",
    "            token_features['word_shape#' + word_shape] = 1 #word shape\n",
    "            \n",
    "            features.append(token_features)\n",
    "            token_count += 1\n",
    "    return features\n",
    "\n",
    "#vectorizing features\n",
    "vectorizer = DictVectorizer(sparse = True)\n",
    "x = vectorizer.fit_transform(featurizer(sentences))\n",
    "\n",
    "#class extraction function\n",
    "def classizer(sentences): \n",
    "    classes = []\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            classes.append(token[3])\n",
    "    return classes\n",
    "\n",
    "y = classizer(sentences)\n",
    "\n",
    "#training classifier\n",
    "classifier = svm.LinearSVC()\n",
    "classifier.fit(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running classifier on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file = readconll('eng.testa')\n",
    "\n",
    "to_predict_test = vectorizer.transform(featurizer(test_file)) #vectorizing test data features\n",
    "predicted_classes_test = classifier.predict(to_predict_test) #predicting classes for test set\n",
    "\n",
    "\n",
    "i = 0 #appending predicted classes to test data\n",
    "for sentence in test_file:\n",
    "    for token in sentence:\n",
    "        token.append(predicted_classes_test[i])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Writing test data with predicted classes to file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"eng.guessa\", \"w\") as variable_file:\n",
    "    for sentence in test_file:\n",
    "        for token in sentence:\n",
    "            variable_file.write(\" \".join(token))\n",
    "            variable_file.write(\"\\n\")\n",
    "        variable_file.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
